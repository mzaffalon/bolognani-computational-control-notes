\chapter{Monte-Carlo Learning}
\label{chap:mc-learning}

Also in the context of MDP, one is interested in finding the optimal policy without prior model information. There are two settings:
\begin{itemize}
\item based on collected data, typically in the form of repeated episodes
  \begin{equation*}
    (x_0,u_0,r_0),\ (x_1,u_1,r_1),\ldots (x_T,u_T,r_T)
  \end{equation*}
  for instance coming from repetition of the control task in a numerical simulator or controlled environment, \textit{i.e.} experimentally. The episodes could be stochastic because the application of $u_0$ from state $x_0$ may transition the system to a different final state but the collected data will reveal only one realization;
\item online during the control task with no prior training in a one-shot approach. The \emph{adaptive control} approach is a challenging dual learn and control task and has been an open problem for decades.
\end{itemize}
The first approach is suitable for an incremental improvement of the policy; the second can deal with time-dependent models.

\section{$Q$ Function}
\label{sec:Q-function}

We have seen that $V^\pi(x)$ is a predictor of the quality of the candidate policy. For reinforcement learning where the learning comes from experimental data, it is more convenient to reformulate the problem in terms of the quality $Q$ function
\begin{equation}
  \label{eq:q-function-definition}
  Q^\pi(x,u) = R_x^u + \gamma \sum_{x^\prime}P_{x,x^\prime}^u V^\pi (x^\prime).
\end{equation}
$Q^\pi(x,u)$ is the expected future cost when taking action $u$ in state $x$, as dictated by the policy $\pi$. In contrast to the value function $V(x)\in \mathbb{R}^N$, the size of $Q$ is $N\times M$: this increased problem size has the advantage, amongst others, to make the policy update step of the policy iteration trivial.

The value function is the expected $Q$ function averaged over the policy $\pi$
\begin{equation}
  \label{eq:value-function-from-q}
  V^\pi(x) = \sum_u \pi(x,u)Q^\pi(x,u).
\end{equation}
The Bellman equation can be written in terms of the $Q$ function in closed form using eq.~\eqref{eq:value-function-from-q} in eq.~\eqref{eq:q-function-definition}
\begin{equation}
  \label{eq:Q-function-policy-evaluation}
  Q^\pi(x,u) = R_x^u + \gamma \sum_{x^\prime} P_{x,x^\prime}^u \sum_{u^\prime} \pi(x^\prime,u^\prime)Q^\pi(x^\prime,u^\prime)
\end{equation}
and the Bellman optimality principle as
\begin{equation}
  \label{eq:Q-function-Bellman-optimality}
  Q^\star(x,u) = R_x^u + \gamma \sum_{x^\prime}P_{x,x^\prime}^u \left(\min_{u^\prime} Q^\star(x^\prime,u^\prime)\right)
\end{equation}
where here we used the previous observation that the optimal policy is deterministic.

\subsection{Policy Iteration for the $Q$ function}

The policy iteration with the $Q$ function is rewritten as
\begin{itemize}
\item the \emph{policy evaluation} requires solving the higher-dimensional linear system eq.~\eqref{eq:Q-function-policy-evaluation};
\item the \emph{policy update} becomes even simpler than for the value function
  \begin{equation*}
    \pi(x,u) = \arg \min_\nu \sum_u \nu(x,u) Q(x,u).
  \end{equation*}
\end{itemize}
Note that now only the policy evaluation requires the model information which is contained in the large matrix $Q(x,u)$.

We are now left with the task of computing the policy assessment step: the advantage of this approach is that this can be obtained from the simulated data:
\begin{itemize}
\item let the system be controlled by a policy $\pi$ and collect a sufficiently long episode
  \begin{equation*}
    (x_0,u_0,r_0),\ (x_1,u_1,r_1),\ldots (x_T,u_T,r_T);
  \end{equation*}
\item compute the \emph{empirical cost} $g_0$, $g_1$,\ldots $g_T$ from the immediate costs $r_i$ and the discount cost factor $\gamma$
  \begin{equation}
    \label{eq:RL-empirical-cost}
    g_k = \sum_{i=k}^T \gamma^{i-k}r_i;
  \end{equation}
\item interpret $g_k$ as a realization of the return\footnote{By definition, $Q^\pi(x,u)$ is the cost of applying input $u$ when in state $x$ and then applying the policy $\pi$.} of the state-action pair $(x_k,u_k)$
  \begin{equation*}
    Q^\pi(x_k,u_k) \approx g_k.
  \end{equation*}
  If the system is stochastic and therefore governed by transition probabilities, $Q^\pi(x,u)$ must be computed as the average over multiple visits
  \begin{equation}
    \label{eq:RL-Q-estimate-multiple-visits}
    Q^\pi(x,u) = \frac{\sum_{t=1}^T 1[x_t=x,u_t=u]g_t}{\sum_{t=1}^T 1[x_t=x,u_t=u]}
  \end{equation}
  where the accumulation must be sufficiently long to see all possibles combinations of states and inputs.
\end{itemize}
This method assumes no explicit knowledge of transition probabilities $P_{x,x^\prime}^u$.

A few remarks are required:
\begin{itemize}
\item the policy cannot be evaluated, and therefore $Q$ cannot be computed, until the episode is terminated. This is because the empirical cost eq.~\eqref{eq:RL-empirical-cost} requires one to use the same policy until the end of the episode. MC learning method is therefore not adequate to adaptive learning;
\item there is no guarantee that the \emph{estimate} of $Q$, \textit{i.e.} for a single realization, eq.~\eqref{eq:RL-Q-estimate-multiple-visits}, satisfies the self-consistent Bellman equation eq.~\eqref{eq:Q-function-policy-evaluation}. The identity is however valid in the limit of infinite realizations;
\item Markovianity? I did not understand the remark in the lecture.
\end{itemize}

\subsection{Exploration vs Exploitation}

The greedy policy improvement step can be stuck in a suboptimal solution when the triple $(x,u,r)$ is never explored by the policy and the associated $Q$ value is not available. This is usually fixed by \textit{e.g.}
\begin{itemize}
\item the $\epsilon$-greedy policy improvement, where a random action from all possible actions $\mathcal{U}$ is tried with probability $\epsilon$
  \begin{equation}
    \label{eq:epsilon-greedy-policy-improvement}
    \pi(x,u) =
    \begin{cases}
      \arg \min_u Q(x,u) & \text{with probability }1-\epsilon \\
      \text{Uniform}(\mathcal{U}) & \text{with probability }\epsilon
    \end{cases}.
  \end{equation}
  A large $\epsilon$ allows for more exploration; or
\item the Bolzmann policy improvement\footnote{A note on the initialization of the $Q$ function: the Bolzmann policy requires the knowledge of its values also for state-action pairs that have not been explored. If the optimization is set up to minimize positive costs, $Q$ is initialized to a large number; in the case of maximization, as it is more common in RL settings, then $0$ is commonly used.}
  \begin{equation*}
  \pi(x,u) = \frac{e^{-\beta Q(x,u)}}{\sum_u e^{-\beta Q(x,u)}}.
\end{equation*}
Here a small $\beta$ implies more exploration.
\end{itemize}
In both cases, if done ``properly'', each state-action pair is visited infinitely many times and the policy converges to a greedy policy with probability $1$.

\section{Scalability and Parametrization of $Q$}
\label{sec:RL-Q-parametrization}

The computational complexity of the $Q$ function becomes intractable when the state-action space is large, either because the system has intrinsically many states and inputs or as a result of the discretization of a continuous system. The trick is to parametrize the $Q$ function by a small set of basis functions
\begin{equation*}
  Q_\theta(x,u),\quad \theta \in\mathbb{R}^d\quad \text{where } d\ll MN
\end{equation*}
thereby restricting unnecessary degrees of freedom. A smart parametrization may allow one to guess the $Q$ function also for state-action pairs that have not been observed. For continuous systems the basis functions are continuous functions and the summation in the Bellman equation must be replaced by integrals; nevertheless the parametrization transforms the initial system into a discrete one.

Prior information on the problem may suggest a smart parametrization. For instance in the LQR case, we assumed that the value function is quadratic in $x$ and $u$ (?). Did not understand the remark from the lecture.

The downside of the parametrization is that the optimal $Q^\star$  may not belong to the lower dimensional space. Therefore the solution to the Bellman equation for a given policy $\pi$ may not belong to the space
\begin{equation*}
  \mathcal{Q} = \{Q_\theta,\ \theta\in\mathbb{R}^d\}
\end{equation*}
and policy iteration may not converge or converge to the suboptimal solution. Moreover the policy update step may require additional computation that in the discrete case was limited to selecting one entry in the Q function.

\subsection{Linear Parametrization}
\label{sec:RL-linear-parametrization}

We consider the linear parametrization
\begin{equation}
  \label{eq:Q-linear-parametrization}
  Q_\theta(x,u) = \sum_{\ell=1}^d \phi_\ell(x,u)\theta_\ell = \Phi(x,u)\theta
\end{equation}
where $\phi_\ell(x,u)$ are basis functions. These can be
\begin{itemize}
\item polynomials $\{1, x, u, x^2, u^2, xu,\ldots\}$;
\item quadratic forms $x^\top A_\ell x$ where $A_\ell$ can be a matrix with only one element different from zero or a set of positive definite matrices;
\item a set of discrete basis functions\footnote{In the game of Tetris on a board size of $20\times 10$, there are at least $2^{200}$ states, each board position being either empty or occupied. A solution mimicking the human approach has been realized with $d=21$ only.}.
\end{itemize}

In general the solution to the Bellman equation eq.~\eqref{eq:Q-function-policy-evaluation} is not in the form $\Phi\theta$ and the equation can only be solved approximately
\begin{equation*}
  Q_\theta(x,u) \approx \underbrace{R_x^u + \gamma\sum_{x^\prime} P_{x,x^\prime}^u \sum_{u^\prime} \pi(x^\prime,u^\prime)Q_\theta(x^\prime,u^\prime)}_{:=\mathcal{B}(Q_\theta)}
\end{equation*}
where, in matrix form,
\begin{equation}
  \label{eq:Q-function-matrix}
  \mathcal{B}(Q) = R + \gamma T^\pi Q.
\end{equation}
The \emph{projected Bellman equation} seeks a solution $Q_\theta$ by first projecting $\mathcal{B}(Q_\theta)$ into the subspace $\mathcal{Q}$ using the projection operator
\begin{equation*}
  \Pi(\mathcal{B}(Q_\theta)) = \arg \min_{Q_{\hat{\theta}}}\ |\!| Q_{\hat{\theta}} - \mathcal{B}(Q_\theta) |\!|_\rho^2
\end{equation*}
in the $\rho$-norm\footnote{The $\rho$-norm is defined as
\begin{equation*}
  |\!|v|\!|_\rho^2 = v^\top \rho v.
\end{equation*}}
(where is the $\rho$ matrix coming from?) and then looks for the fixed point
\begin{equation}
  \label{eq:projected-Bellman-equation}
  Q_\theta = \Pi(\mathcal{B}(Q_\theta)).
\end{equation}

\begin{theorem}
  The fixed point of the projected Bellman equation is given by
  \begin{equation}
    \label{eq:Q-function-projected-solution}
    %\left(\Phi^\top \rho \Phi - \gamma \Phi^\top\rho T^\pi\Phi\right)\theta = \Phi^\top\rho R
    \Phi^\top \rho \left(I - \gamma T^\pi\right) \Phi\theta = \Phi^\top\rho R.
  \end{equation}
\end{theorem}
\begin{proof}
  Let $Q_\theta = \Phi \theta$; by eq.~\eqref{eq:Q-function-matrix} we have $\mathcal{B}(Q_\theta) = R + \gamma T^\pi\Phi\theta$. To find $\Pi(B(Q_\theta))$, we compute
  \begin{equation*}
    %\arg \min_{\hat{\theta}} \frac{1}{2} |\!| \Phi \hat{\theta} - \mathcal{B}(Q_\theta) |\!|^2_\rho =
    \arg \min_{\hat{\theta}} \frac{1}{2} |\!| \Phi \hat{\theta} - R - \gamma T^\pi \Phi\theta |\!|^2_\rho
  \end{equation*}
  by setting the gradient with respect to $\hat{\theta}$ to zero, to give
  \begin{equation*}
    % \Phi^\top\rho\left(\Phi \hat{\theta} - R - \gamma T^\pi \Phi\theta\right) = 0
    \Phi^\top\rho\Phi \hat{\theta} = \Phi^\top\rho\left(R + \gamma T^\pi \Phi\theta\right).
  \end{equation*}
  and  $\Pi(B(Q_\theta)) = \Phi\hat{\theta}$. Imposing the fixed point condition eq.~\eqref{eq:projected-Bellman-equation} gives
  \begin{equation*}
    \Phi\hat{\theta} = \Phi\theta
  \end{equation*}
  which we can substitute into the equation above. After rearranging the terms, we obtain eq.~\eqref{eq:Q-function-projected-solution}.
\end{proof}

Eq.~\eqref{eq:Q-function-projected-solution} needs the model information contained in $T^\pi$. For a model free approach, we construct the matrices $\Phi^\top\rho \Phi$, $\Phi^\top\rho \Phi$ and $\Phi^\top\rho R$ from the episodes/trajectories. From the data and the basis function, one can construct the terms
\begin{equation*}
  \phi(x_k,u_k)\phi(x_k,u_k)^\top,\quad \phi(x_k,u_k)\phi(x_{k+1},u_{k+1})^\top,\quad \phi(x_k,u_k)r_k
\end{equation*}

This part was not clear from the lecture. $\rho$ depends on how often the pair $(x,u)$ is visited.

\begin{theorem}
  We have
  \begin{equation*}
    \Phi^\top\rho\Phi \approx \frac{1}{K} \sum_{k=1}^K \phi(x_k,u_k)^\top\phi(x_k,u_k).
  \end{equation*}
\end{theorem}
\begin{proof}
  ?
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
