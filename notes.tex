\documentclass[11pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage{biblatex}
\date{\today}
\title{}

\newcommand{\du}{\ensuremath{\mathrm{d}}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\bsu}{\bs{\mathrm{u}}}
\newcommand{\bsx}{\bs{\mathrm{x}}}

\newtheorem{theorem}{Theorem}

\begin{document}

\tableofcontents

%\setcounter{chapter}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{Dynamic Programming and LQR}
% \label{sec:dynamic-programming-lqr}

% Consider the translunar coast with the following performance metric:
% minimization of fuel use, circumnavigation of the Moon, reenter Earth
% by time $T$. We need to compute the sequence
% $\bsu = u_0, u_1,u_u,\ldots$.

% The optimization problem is
% \begin{equation*}
%   \min_{\bsu\in\mathcal{U}} J(\bsu)
% \end{equation*}
% where $\mathcal{U} = \tilde{\mathcal{U}} \times \{0,1,2,\ldots,T\}$
% and the cost is $J(\bsu)$ that includes fuel cost $\bsu$ and terminal
% constraint $\delta(\bsu)$ that is applied only at the final state
% $x_T$
% \begin{equation*}
%   \delta(\bsu) =
%   \begin{cases}
%     0 & \text{ if } \\
%     \infty & \text{ otherwise}
%   \end{cases}
% \end{equation*}

% If the problem can be written in the form

% \begin{align*}
%   \min_{\bsx,\bsu} \sum_{t=0}^T g_t(x_t,u_t) \\
% \end{align*}
% then Bellman's principle can be used to solve the optimization
% problem.

% is in general an untractable optimization problem for the following
% reasons:
% \begin{itemize}
% \item the dimension of the space $\mathcal{U}$ make the problem size
%   formidable;
% \item the cost function requires an accurate model of the system,
%   which can be obtained by first-principle model, numerical simulation
%   and black box/oracle for the terminal constraint;
% \item the optimization problem is non-convex. In the case of the
%   translunar coast, assume two control inputs that make the rocket
%   orbit the moon once and twice respectively. A linear combination of
%   these two trajectories may not make the rocket to Earth;
% \item the open-loop nature of the optimal control problem makes it
%   useless in the presence of disturbances and uncertainty.
% \end{itemize}



% Given the Markovian update
% \begin{equation*}
%   x_{t+1} = Ax_t + Bu_t
% \end{equation*}
% and the cost function
% \begin{equation*}
%   \sum_{t=0}^{T-1} \left(x_t^\top Qx_t + x_t^\top Qx_t\right) + x_T^\top Sx_T,\quad Q,S \succeq 0, R \succ 0
% \end{equation*}
% which has solution
% \begin{equation}
%   \label{eq:solution-timedependent-ricatti}
%   P_t = Q + A^\top P_{t+1}A - A^\top P_{t+1}B\left(R+B^\top P_{t+1}B\right)^{-1}B^\top P_{t+1}A
% \end{equation}

% Infinite horizon algebraic Ricatti equations are solved by using a
% solver that selects the solution with the \emph{largest} norm. They
% can also inefficiently be computed by iterating $P_t$ backward. See
% the example below.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Stability of the Optimal Controller}
% \label{sec:stability-optimal-controller}

% Optimality of the controller is with respect to the cost function, but
% the cost function may not make the system stable. Consider the example
% \begin{equation*}
%   x_{t+1} =
%   \begin{bmatrix}
%     2 & 0 \\ 0 & 3
%   \end{bmatrix}x_t + u_t, \qquad Q =
%   \begin{bmatrix}
%     1 & 0 \\ 0 & 0
%   \end{bmatrix}, \quad R =
%   \begin{bmatrix}
%     1 & 0 \\ 0 & 1
%   \end{bmatrix}, \quad u_t = \Gamma_tx_t
% \end{equation*}
% The system has eigenvalues outside of the unit circle. The optimal
% solution does not stabilize the system, since it must control the
% second input for which there is a cost $R_{2,2}\neq 0$ but zero cost
% to let the state diverge, $Q_{2,2}=0$.

% Working through the Ricatti equation gives a diagonal matrix whose
% entries and solutions are
% \begin{align*}
%   1 + 3p_1 - \frac{4p_1^2}{1+p_1} &\rightarrow 2\pm \sqrt{5} \\
%   8p_2 - \frac{9p_2^2}{1+p_2} &\rightarrow 0, 8
% \end{align*}
% The stabilizing optimal solution (but clearly not the one with minimal
% norm?) is
% \begin{equation*}
%   P =
%   \begin{bmatrix}
%     2 - \sqrt{5} & \\ 0 & 8
%   \end{bmatrix}
% \end{equation*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Optimal Control beyond LQR}
\label{sec:optimal-control-beyond-lqr}

Model Predictive Control (MPC) is a discrete process control where the control input $\bsu = \{u_0,u_1,\ldots,u_{K-1}\}$ and the states $\bsx$ must satisfy a set of contraints:
\begin{equation}
  \label{eq:MPC-general-formulation}
  \begin{aligned}
    J(X_0) = \min_{\bsx,\bsu} &\sum_{k=0}^{K-1} g_k(x_k,u_k) + g_K(x_K),\\
    \text{subject to } & x_{k+1} = f(x_k,u_k) \\
                                                       & x_0 = X_0 \\
                                                       & x_k \in \mathcal{X}_k,\ u_k \in \mathcal{U}_k
  \end{aligned}
\end{equation}
where $g_k$ is a non-negative cost function such as a quadratic one
\begin{equation}
  \label{eq:quadratic-stage-cost}
  g_k(x,u) = x^\top Q_k x + u^\top R_k u.
\end{equation}
In the presence of constraints, one cannot find the optimal solution by setting the gradients $\nabla_{u_k} V$ of the cost-to-go
\begin{equation}
  \label{eq:cost-to-go}
  V(\bsx,\bsu) = \sum_{k=0}^{K-1} g_k(x_k,u_k) + g_K(x_K)
\end{equation}
to zero as it was the case for the LQR problem. The optimal control input sequence $\bsu^\star = \{u_0^\star,u_1^\star,\ldots,u_{K-1}^\star\}$ and the optimal trajectory $\bsx^\star = \{x_0^\star,x_1^\star,\ldots,x_K^\star\}$ can however be found if the optimization problem is feasible. In open-loop control, the whole optimal sequence of input control inputs $\bsu^\star$ is applied and the system evolves to reach the terminal state $x_K$ after $K$ steps.

In case of model mismatch or state perturbation however, the system will deviate increasingly more from the predicted trajectory $\bsx^\star$. If a measurement of the system state is available, it makes sense to find a new optimal control input vector using the measured state as the initial point for the optimization. (Question: if this is the case, then why not optimizing only the tail such that also in closed loop the trajectory is completed after $K$ steps? )

Closed-loop control applies the following strategy: at time step $i$ with the system in state $x_i$, compute the optimal control input $\bsu_i^\star=\{u_{i,0}^\star,u_{i,1}^\star\ldots,u_{i,K-1}^\star\}$, apply the first element of it, $u_{i,0}^\star$, and discard the remaining elements. At the following time step $i+1$, with the system having evolved into state $x_{i+1}$, solve again the optimization with initial state $x_{i+1}$ to find the optimal control input $\bsu_{i+1}^\star$, apply the first element $u_{i+1,0}^\star$ and discard the remaining elements. This process is repeated.

The closed-loop approach permits us to solve the optimization problem with an horizon of length $K$ in a receding way, by restarting the control process from the current state. This is possible since the system is Markovian and the current state is all one needs to compute the optimal control for the future.

If the model closely represents the system and in case of perturbations, the closed-loop approach can be restarted using the predicted state $x_{k+1} = f(x_k,u_k$).

Either way, closed-loop MPC solves repeatedly the exact same problem, parametrised by the initial state $x_0=x$
\begin{align*}
  \bsu^\star(x) \text{ determined by } \min_{\bsu, \bsx} &\ \sum_{k=0}^{K-1}g_k(x_k, u_k) + g_K(x_K)\\
  \text{subject to } & x_{k+1} = f(x_k,u_k)\quad k=0,\ldots,K-1 \\
                                                         & x_0 = x \\
                                                         & x_k \in \mathcal{X}_k,\ u_k \in \mathcal{U}_k.
\end{align*}
Since the problem is the same, the optimal solution $\bsu^\star$ and in particular its first element $u_0^\star$ is independent from the time index $i$ and the index will be dropped in the following.

In general there is no guarantee that the map $x \rightarrow u_0^\star(x)$ is a continuous function even under the assumption of the continuity of $f$ and $g_k$; the cost-to-go $V(\bsx,\bsu)$ is instead continuous in the parameters $\bsx$ and $\bsu$. Moreover under compactness assumptions on the constraints, the parametric optimization problem has a solution when it is feasible (I do not understand this sentence).

From a computational point of view, the problem eq.~\eqref{eq:MPC-general-formulation} can be efficiently solved if it is convex, which happens for instance if the stage cost $g_k$ is a convex function, the spaces $\mathcal{X}_k$ and $\mathcal{U}_k$ are convex and the dynamics is linear $f(x_k,u_k)=Ax_k+Bu_k$.

%In the following we will assume that the problem is convex and the dynamics is linear.

To summarize: MPC is a static\footnote{A static controller has no memory of the past and depends only on the current statte $x_k$:
  \begin{equation*}
    u_k = \varphi_k(x_k)
  \end{equation*}
  whereas a dynamic controller has memory, such as in the case of a
  PI controller
  \begin{equation*}
    \xi_{k+1} = \xi_k + \varphi_k(x_k),\quad u_k = \psi_k(x_k) + \xi_k.
  \end{equation*}}, nonlinear, time-invariant feedback\footnote{Because $f$ does not depend on $k$.} control-law.


\subsection{Does MPC Solve the Optimal Control Problem?}
\label{sec:MPC-not-an-optimal-control-solver}

The 
\emph{This part in the video is not clear.}

MPC does not solve the global optimization control problem, because it
discards

Consider the problem
\begin{align*}
  \min_{\bsu,\bsx} & \sum_{k=0}^{K-1}\frac {1}{k}||u_u|| + ||x(K)||^2\\
  \text{subject to } &x_{k+1} = x_k+u_k
\end{align*}
the control action computed by MPC will be zero until step $K-1$ and
then??? Not clear here.

\subsection{MPC and Finite-Time LQR}
\label{sec:MPC-finite-time-LQR}

I did not understand it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation of MPC}
\label{sec:implementation-MPC}

An optimal MPC trajectory can be computing using an offline or an online approach.

\subsection{Offline Computation of the Optimal Control Input}
\label{sec:offline-computation-MPC}

The offline method computes the \emph{function} $u_0^\star(\cdot)$. This may require gridding of the solution and is a complex problem because of the exponential number of the number of regions where the solution must be partitioned. In the following section, we will solve a toy problem with time horizon $K=1$ using the KKT condition.

\subsubsection{Short Introduction to the KKT Condition}
\label{sec:short-intro-KKT}

This section is a minimal review of the KKT condition. The minimization problem
\begin{align*}
  \min_x &\ f(x) \\
  \text{subject to } &\ g(x) \le 0
\end{align*}
is equivalent to
\begin{align*}
  \begin{cases}
    \nabla f(x) + \sum_i \mu_i \nabla g_i(x) = 0 \\
    g(x) \le 0 \\
    \mu_i g_i(x) = 0\ \forall i
  \end{cases}
\end{align*}
with $\mu_i\ge 0$ and provided $f$ and $g$ are differentiable.

The condition breaks down into the following two cases:
\begin{itemize}
\item the minimum $\bsx^\star$ is inside the feasible domain as   detemined by $g_i(\bsx^\star) < 0$, in which case $\mu_i=0$ and   $\nabla f(\bsx^\star) = 0$. This is the usual case where the minimum   is found by setting the gradient to zero and the constraints have no   effect, or
\item the minimum occur at the boundary $g_i(\bsx^\star) = 0$,   $\mu_i\neq 0$ and the two vectors $\nabla f(\bsx^\star)$ and   $\sum_i \nabla g_i(\bsx^\star)$ are parallel.
\end{itemize}
The behaviour is qualitatively different whether the minimum is inside or at the boundary.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Offline computation: a Simple Minimization Problem}

Let us consider the following toy example:
\begin{align*}
  \min_{u_0,x_1} &\ u_0^2+x_1^2 \\
  \text{subject to } &\ x_1 = x_0+u_0 \\
                 &\ x_1\le 1.
\end{align*}
For such simple cases, the KKT condition is all one needs to find minimizers. Eliminating $x_1$ gives the minimization problem
\begin{align*}
  \min_{u_0}\  & f(u_0) \\
  \text{subject to } & g(u_0) \le 0
\end{align*}
where $f(u_0) = u_0^2 + (x_0+u_0)^2$, $g(u_0) = x_0 + u_0 - 1$ and $\nabla_{u_0} f(u_0) + \mu \nabla_{u_0} g(u_0) = 4u_0 + 2x_0 + \mu$. We consider the two cases
\begin{itemize}
\item $\mu = 0$: we have
  \begin{align*}
    & 4u_0 + 2x_0 = 0 \rightarrow u_0 = -\frac{x_0}{2} \\
    & x_0 + u_0 - 1 < 0 \rightarrow x_0 < 2.
  \end{align*}
\item $\mu > 0$: from $\mu g(x) = 0$ the minimum is at the boundary $g(x)=0$ and $4u_0+2x_0+\mu > 4u_0+2x_0$ since $\mu>0$. We have
  \begin{align*}
    &x_0 + u_0 - 1 = 0 \rightarrow u_0 = 1-x_0 \\
    &4u_0 + 2x_0 = 4(1-x_0) + 2x_0 > 0 \rightarrow x_0 > 2.
  \end{align*}
\end{itemize}
Note that the solution is continuous at $x_0=2$: we have therefore the solution
\begin{equation*}
  \begin{cases}
    u_0 = -\frac{x_0}{2} & \text{ for } x_0 < 2 \\
    u_0 = 1-x_0 & \text{ for } x_0 \ge 2.
  \end{cases}
\end{equation*}
Each constraint defines two (affine) regions and for each region there is a different control action. If there are $n$ constraints there will be at most $2^n$ regions, according to the way the constraints overlap. For a large number of constraints, the problem becomes intractables also for offline computation. In practice, one problem to solve is to figure out where in the state space one is.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Online Computation of the Optimal Control Input}
\label{sec:online-computation-MPC}

Online computation computes the \textbf{value} $u_0^\star(\cdot)$ at every iteration and therefore it is typically applied to processes that have constraints and mainly used in slow dynamics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stability}
\label{sec:mpc-stability}

MPC is a principled way to design static, time-invariant, non-linear feedback control law. While local stability can be analyzed by linearizing the control law around the equilibrium and using the techniques from Control Systems (Nyquist, Bode,\ldots), the tool to investigate global stability of non-linear systems is the Lyapunov theorem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lyapunov Stability}
\label{sec:lyapunov-stability}

Definition: The point $x=0$ is stable for the dynamics $x_{k+1} = f(x_k)$ if a small perturbation of the state perturbs the subsequent state trajectory in a continuous manner.

In mathematical terms,
\begin{equation}
  \label{eq:lyapunov-stability-stable-equilibrium}
  \forall \epsilon > 0,\ \exists \delta > 0 \text{ such that } ||x_0|| < \delta \rightarrow ||x_k|| < \epsilon\ \forall k \ge 0.
\end{equation}
In other words, given a ball of radius $\epsilon$, any trajectory starting from within a ball of radius $\delta$ will never leave the larger ball with radius $\epsilon$.

An equilibrium is called \emph{asymptotically stable} if it is stable and $\lim_{k\rightarrow \infty} ||x_k||=0$.

\begin{theorem}[Lyapunov theorem]
  \label{th:lyapunov}
  Given a discrete system with dynamics $x_{k+1} = f(x_k)$ and equilibrium $x=0$ (?), if $W(x)$ is a real-valued function such that
  \begin{align*}
    W(0)=0 \text{ and } W(x)>0\quad \forall x\neq 0 \\
    W(f(x)) < W(x)\quad \forall x\neq 0
  \end{align*}
  then the point $x=0$ is asymptotically stable.
\end{theorem}
A survey of formulation of Lyapunov theory for discrete systems is available at \url{https://arxiv.org/abs/1809.05289}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stability of MPC}
\label{sec:stability-MPC}

To prove asymptotically stability of the closed loop problem\footnote{Note that the open loop is trivially asymptotically stable because it consists of a sequence of finite length $K$.}, we need to find a Lyapunov function $W(x)$ that satisfies theorem~\ref{th:lyapunov}, for the equilibrium point $(x_S,u_S)=(0,0)$.

We consider first the simpler case of stability of the infinite horizon unconstrained problem. The presence of constraints does not alter the validity of the proof. The function
\begin{equation*}
  W(x) = \min_{\bsx,\bsu} \sum_{k=0}^\infty g_k(x_k,u_k).
\end{equation*}
is a candidate for the Lyapunov function provided that 1. $W(x)$ is only zero at the target equilibrium (such as when $g_k$ is a quadratic objective function) and 2. that it decreases along trajectories of the system. This is the case because the MPC controller finds $(\bsx^\star, \bsu^\star)$ that minimizes the problem
\begin{equation*}
  V^\infty_i(\bsx,\bsu) \equiv \sum_{k=i}^\infty g_k(x_k,u_k).
\end{equation*}
By Bellman's principle, the optimal trajectory minimizes also the ``tail'' cost starting at the subsequent time $i+1$
\begin{equation*}
  V_{i+1}^\infty(\bsx,\bsu) = \sum_{k=i+1}^\infty g_k(x_k,u_k)
\end{equation*}
and therefore we have that
\begin{equation*}
  W(x_{i+1}) = W(x_i) - g_i(x_i,u_0^\star(x_i)) < W(x_i).
\end{equation*}
This poses the restriction than $g_k$ is a strictly positive quantity: if a state $\bs{\bar{x}}$ is not weighted in the cost function and $g_k(\bs{\bar{x}})=0$, then $W(x)$ defined above is not strictly decreasing and would not be a Lyapunov function for the asymptotic stability test.

For a finite receding horizon MPC problem with horizon $K$, the trick above does not apply because at $i+1$, the new cost to go includes the term $g_{K}()$ and excludes $g_k$
\begin{align*}
  V^K_{k+1}(\bsu,\bsx) &= \sum_{s=k+1}^{k+K+1}g_s(x_s,u_s) \\
                       &= V_k(\bsu,\bsx) + g_{k+K+1}(x_{k+K+1},u_{k+K+1}) - g_k(x_k,u_k)
\end{align*}
and there is no guarantee that the requirement
\begin{equation*}
  \min_{\bsu,\bsx} V^K_{i+1} < \min_{\bsu',\bsx'} V^K_i
\end{equation*}
is satisfied.

We can however construct a new optimization problem equal to the original one to which we add the constraint $x_K=0$ on last state $x_K$
\begin{equation}
  \label{eq:lyapunov-tracking-MPC}
  \begin{aligned}
    W(x) \qquad\quad\equiv \min_{\bsu,\bsx} &\quad \sum_{k=0}^{K-1} g_k(x_k,u_k) \\
    \text{subject to } &\quad x_{k+1} = f(x_k,u_k) \\
                                            &\quad  x_0 = x,\ x_K = 0
  \end{aligned}
\end{equation}
following the intuition that, because one is trying to stabilize the system, the final state will be close to zero. This is a Lyapunov function if we prove that this is a decreasing function along a trajectory.

Assume we found the optimal solution $\bsu_{k+1}^\star$ to the modified problem for $k+1$: this has a lower cost than the suboptimal solution $\bsu_{k+1}$ for step $k+1$ obtained by taking the optimal solution $\bsu_k^\star$ for step $k$, removing state $u_k$ and adding $u_{k+K+1} = 0$ such that $x_{k+K+1} = 0$. The suboptimal solution $\bsu_{k+1}$ conversely has a lower cost than the optimal solution $\bsu_k^\star$ because we have removed the cost $g_k(x,u_k)$:
\begin{align*}
  W(x_{k+1}) &= V_{k+1}^K(\bsx_{k+1}^\star, \bsu_{k+1}^\star) & \text{definition of $W$} \\
             & \leq V_{k+1}^K(\bsx_{k+1}, \bsu_{k+1}) & \text{definition of optimal trajectory} \\
             & = \underbrace{V_k^K(\bsx_k^\star, \bsu_k^\star)}_{W(x_k)} - g_k(x_k^\star,u_k^\star) + \\
             & \underbrace{g_{k+K+1}(x_{k+K+1},u_{k+K+1})}_{=0} & \text{definition of $V_k^K$} \\
  & < W(x_k)
\end{align*}
This proves that $W$ is a decreasing function on a trajectory.

The infinite horizon problem is computationally impossible to solve, unless the system has special properties. The receding MPC problem with horizon $K$ is a computationally tractable approximation to the inifite horizon problem, the approximation being good provided $K$ is chosen large enough that the interesting dynamics has finished.

MPC problems are usually set up by adding a final cost or constraints conditions on the final state: since the point $(x,u)=(0,0)$ is an equilibrium, one can impose the constraint$x_K=0$ on the last state. Alternatively, one can impose $x_K$ live in a small space $\mathcal{X}_K$, the reason being that if there is a region of the state space sufficiently close to equilibrium that is feasible, the trajectory starting at time $k+1$ will keep the system feasible and hopefully easy to control. Another possiblity is to approximate the tail of the infinite horizon problem from element $K+1$ onwards by a large final cost $g_K(x_K)$ if the final state is far from the equilibrium.

We have assumed feasibility (needs to be define or/and give an example of problem that becomes unfeasible) of the MPC problem which may break in the absence of a zero terminal constraint.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Steady-State Selection}
\label{sec:steady-state-selection}

Until now we have assumed we want to regulate at the steady-state $(x_s,u_s) = (0,0)$. There are sometimes situations where we want the plant to operate at a working point $x_s\neq 0$ and to allow a constant input $u_s$.

If the desired equilibrium $(x_s, u_s)$ is known in advance,


I can set up the problem in the original variables with the stage cost being $g(\bsx,\bsu) = ||\bsx-\bsx_s||_Q + ||\bsu-\bsu_s||_R$


\subsection{Steady-State Selection from Specifications}
\label{sec:steady-state-from-specs}

Sometimes the problem specifies a working point $(x_\text{spec},u_\text{spec})$ so selected because it is safe and/or economic to operate. This working point may not be a valid state, \textit{i.e} it does not satisfy $x_\text{spec} = Ax_\text{spec}+Bu_\text{spec}$. To find the steady-state, one solves offline the optimization problem
\begin{align*}
  \min_{u_s,u_s} & ||x_s-x_\text{spec}||_{Q_s}^2 + ||u_s-u_\text{spec}||_{R_s}^2 \\
  \text{subject to} &
                      \begin{bmatrix}
                        I-A & -B
                      \end{bmatrix}
                      \begin{bmatrix}
                        x_s \\ u_s
                      \end{bmatrix} = 0 \\
                 & x_s \in \mathcal{X}\ u_s \in \mathcal{U}.
\end{align*}


\subsection{Robustness to Perturbations}
\label{sec:robustness-to-perturbations}

We have implicitly assumed that we are trying to regulate the steady state $(x_s,u_s) = (0,0)$ but in general, we may be interested in plants that operate at a desiderable working point $x_s\neq 0$. Even in case of perfect plant model $x_{k+1} = f(x_k,u_k)$, a step perturbation cannot be rejected while at the same time reaching the equilibrium point $x=0$ without the presence of an integrator in the plant. Typically an MPC is set up to use input variations $\Delta u_k$ instead of the absolute input $u_k$ only and augments the state space by the control input.

In the truck platoon example, keeping the distance to the previous truck constant cannot be relied on only by knowing the (non-linear) relation between gas pedal angle and truck speed, even in presence of feedback. Instead the gas pedal is adjusted to compensate for variations of parameters (engine efficiency, friction, wind gusts\ldots): the controller adjusts $\Delta u_k$ which is integrated to generate the control input $u_k$ that is then fed to the plant. This has also the practical advantage in that it is easier to implement input weights penalizing \emph{changes} in the input signal.

An integrator may already be present in the plant. For example in drug administration, the body integrates the amount of drug (while at the same time slowly using it). (Should it one in this case also measure the integrated $u_k$ since now this is part of the augmented state $x_k,u_k$?)

\subsection{Disturbance Rejection in MPC}
\label{sec:disturbance-rejection}

% Aside from the MPC controller and the plant, a MPC-based control system may include a state estimator and a steady-state target selection. The estimator estimates the true state based on $(x_k,u_k)$ that are corrupted by the disturbance $d_k$ if the there is a model of the disturbance.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Economic MPC}
\label{sec:economic-mpc}

Many modern control problems present complex performance metrics, such as economic aspects, regulatory specifications and use of resources. Consider an iron furnace, where the inputs are its temperature, the inflow of iron ore, limestone and coke, and the air flow and the products includes the iron yield, its quality, the CO$_2$ emission and the amount of slag. The stage cost $\ell(x,u)$ represents economic losses, energy use, cost of materials, yield\ldots. $\ell$ is a continuous lower-bounded (otherwise the problem is not well defined) function.

One approach is to split the problem into the offline computation of the steady-state computed from the economic cost to go
\begin{align*}
  \min_{x_s,u_s} &\ \ell(x_s,u_s) \\
  \text{subject to} &\ x_s = f(x_s,u_s) \\
                 & x\in \mathcal{X},\ u\in \mathcal{U}
\end{align*}
and an online MPC to track the steady state $(x_s,u_s)$, \textit{e.g.} the optimal furnace temperature, using a quadratic cost function $g_k(x_k-x_s,u_k-u_s)$. This two-stage approach is simpler to analyze but suffers from suboptimal cost of the system trajectory.

Economic MPC uses instead the loss function $\ell$ directly in the dynamic optimization problem
\begin{align*}
  \min_{\bsx,\bsu} & \sum_{k=0}^{K-1} \ell(x_k,u_k) \\
  \text{subject to} &\ x_{k+1} = f(x_k,u_k) \\
                   & x_0 = x,\ x_K=x_s \\
                   & x_k\in \mathcal{X}_k,\ u_k\in \mathcal{U}_k
\end{align*}
eliminating the cascaded optimization by replacing with a single time-scale problem for which the optimization can compute an efficient trajectory, at the cost of a more computationally difficult problem to solve in real-time, since often times is the problem not convex.

Economic MPC is computationally more difficult compared to standard MPC where $g$ is zero at the equilibrium point because there are operating points $(x,u)$ which are not steady states but are economically more convenient:
\begin{equation*}
  \ell(x,u) < \ell(x_s,u_s).
\end{equation*}
To illustrate the situation, consider the furnace example above, starting from an operation point where the furnace is hot and producing molten iron. The controller will the input of coke, thereby reducing the costs but the furnace will keep producing iron for the amount of time its temperature remains above the melting point of ore, longer than the MPC horizon $K$. The point here is that we want the furnace to be economical while at the same time being an operating state for the future.

The economic MPR is naturally an infinite horizon problem because we request long-term profits to be maximised. Moreover an optimal trajectory may not drive the system to a single point equilibrium but rather follow a semi-periodic trajectory. The boundedness is guaranteed by the constraint over the infinite trajectory because by construction, the controller produces an acceptable future trajectory. Such infinite horizon problem problems are computationally very hard.

Solving the problem with a finite horizon is more tractable but because the system is not necessarily driven to equilibrium, we cannot assume that the tail, after the horizon $K$, is small as we did with MPC tracking, where $g$ at equilibrium is zero. Low cost trajectory may not approach equilibrium and unstable trajectory may emerge (cheap trajectory now, expensive to fix in the future). This is the reason why eMPC are solved by constraining the terminal state $x_K=x_s$.

Imposing $x_K=x_s$ induces a final stage cost $g_K(x_s)$ but ensures that the system is still operational at the end of the horizon and not in an unrecoverable state (unfeasible? high cost?). The increased terminal cost $g_K(x_K)$ affects minimally the closed-loop solution (but if it affects minimally, why adding it?).

\subsection{Stability and Performance of Economic MPC}
\label{sec:stability-performance-economic-MPC}

In tracking MPC, the terminal constraint $x_K=0$ was essential in proving that eq.~\eqref{eq:lyapunov-tracking-MPC} was a Lyapunov function for the plant. The key idea was that a valid new trajectory would keep the state close to equilibrium at the tail. However for economic MPC, the same manipulation does not work, since
\begin{equation}
  \label{eq:eMPC-Lyapunov-like-inequality}
  W(x_{i+1}) \le W(x_i) + \ell(x_s,u_s) - \ell(x_i,u_0^\star(x_i))
\end{equation}
is not in general smaller than $W(x_i)$ because to keep the system at steady-state, the cost may exceed the cost of the state after applying control input $u_0^\star(x)$. In contrast with tracking MPC where the stage cost $g_k$ had the property of being zero at the equilibrium point, in economic MPC, $\ell$ is not minimized at the steady state and stronger conditions are required to guarantee asymptotic stability. This is outside the scope of this introduction.

Since eMPC can generate periodic trajectories, are these trajectories better than tracking a point? The following theorem guarantees that trajectories created started from an arbitrary point are more efficient that tracking a steady-state provided the number of time steps $N$ is sufficiently large:
\begin{theorem}
  Let $x_i$ be a closed-loop trajectory generated by the economic MPC controller with terminal constraint $x_s$. Then, for any initial condition
  \begin{equation}
    \label{eq:eMPC-inequality-trajectory-steadystate}
    \limsup_{N\rightarrow \infty} \frac{1}{N} \sum_{k=0}^{N-1} \ell(x_k,u_k) \le \ell(x_s,u_s).
  \end{equation}
\end{theorem}
The proof relies on the identity eq.~\eqref{eq:eMPC-Lyapunov-like-inequality}, rewritten as
\begin{equation*}
  W(x_{i+1}) - W(x_i) \le \ell(x_s,u_s) - \ell(x_i,u_0^\star(x_i))
\end{equation*}
Summing the first $N$ terms of the series and dividing by $N$ gives
\begin{equation*}
  \frac{1}{N}\sum_{i=0}^{N-1} \left[W(x_{i+1}) - W(x_i)\right] = \underbrace{\frac{W(x_N) - W(x_0)}{N}}_{\rightarrow 0 \text{ when } N\rightarrow 0} \le \frac{1}{N}\sum_{i=0}^{N-1}  \left[\ell(x_s,u_s) - \ell(x_i,u_0^\star(x_i))\right]
\end{equation*}
since the quantity $W(x_N) - W(x_0)$ is bounded and after rearranging the terms, we obtain the expression eq.~\eqref{eq:eMPC-inequality-trajectory-steadystate}.

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
